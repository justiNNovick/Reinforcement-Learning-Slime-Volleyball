{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insipired from: https://github.com/hardmaru/slimevolleygym/blob/master/training_scripts/train_ppo_selfplay.py\n",
    "\n",
    "import os\n",
    "import slimevolleygym\n",
    "from datetime import datetime\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from shutil import copyfile # keep track of generations\n",
    "import torch\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from slimevolleygym import BaselinePolicy\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 17\n",
    "NUM_TIMESTEPS = int(5e7)\n",
    "EVAL_EPISODES_SELFPLAY = 100\n",
    "EVAL_EPISODES_BASELINE = 50\n",
    "BEST_THRESHOLD = 0.5 # must achieve a mean score above this to replace prev best self\n",
    "RENDER_MODE = False # set this to false if you plan on running for full 1000 trials.\n",
    "n_cpu = 50\n",
    "EVAL_FREQ = 250000 // n_cpu\n",
    "learning_rate=0.0007\n",
    "n_steps=5\n",
    "gamma=0.99\n",
    "gae_lambda=1.0\n",
    "ent_coef=0.1\n",
    "vf_coef=0.5\n",
    "max_grad_norm=0.5\n",
    "rms_prop_eps=1e-05\n",
    "use_rms_prop=True\n",
    "use_sde=False\n",
    "sde_sample_freq=-1\n",
    "rollout_buffer_class=None\n",
    "rollout_buffer_kwargs=None\n",
    "normalize_advantage=False\n",
    "stats_window_size=100\n",
    "policy_kwargs=None\n",
    "verbose=0\n",
    "device=torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "_init_setup_model=True\n",
    "\n",
    "# Log dir\n",
    "LOGDIR = f\"./Logging/A2C-SELFPLAY-LIBRARY/{datetime.now().strftime('%Y%m%d-%H%M%S')}-lr-{learning_rate}-entcoef-{ent_coef}\"\n",
    "os.mkdir(LOGDIR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=250000, episode_reward=0.20 +/- 2.70\n",
      "Episode length: 649.78 +/- 118.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=500000, episode_reward=-0.10 +/- 2.33\n",
      "Episode length: 666.51 +/- 98.35\n",
      "Eval num_timesteps=750000, episode_reward=0.16 +/- 2.59\n",
      "Episode length: 675.76 +/- 114.76\n",
      "Eval num_timesteps=1000000, episode_reward=-0.51 +/- 2.95\n",
      "Episode length: 592.26 +/- 108.96\n",
      "Eval num_timesteps=1250000, episode_reward=-0.47 +/- 2.41\n",
      "Episode length: 661.00 +/- 95.45\n",
      "Eval num_timesteps=1500000, episode_reward=-0.35 +/- 2.76\n",
      "Episode length: 642.42 +/- 104.32\n",
      "Eval num_timesteps=1750000, episode_reward=0.43 +/- 2.82\n",
      "Episode length: 621.52 +/- 103.42\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000000, episode_reward=0.63 +/- 2.70\n",
      "Episode length: 623.88 +/- 112.57\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2250000, episode_reward=0.25 +/- 2.94\n",
      "Episode length: 611.68 +/- 109.64\n",
      "Eval num_timesteps=2500000, episode_reward=-0.23 +/- 2.84\n",
      "Episode length: 649.18 +/- 128.77\n",
      "Eval num_timesteps=2750000, episode_reward=-0.59 +/- 2.66\n",
      "Episode length: 667.22 +/- 124.55\n",
      "Eval num_timesteps=3000000, episode_reward=-0.12 +/- 2.73\n",
      "Episode length: 638.78 +/- 117.13\n",
      "Eval num_timesteps=3250000, episode_reward=0.03 +/- 2.75\n",
      "Episode length: 634.13 +/- 116.28\n",
      "Eval num_timesteps=3500000, episode_reward=0.00 +/- 2.69\n",
      "Episode length: 637.35 +/- 121.21\n",
      "Eval num_timesteps=3750000, episode_reward=-0.35 +/- 2.85\n",
      "Episode length: 611.82 +/- 105.94\n",
      "Eval num_timesteps=4000000, episode_reward=0.23 +/- 2.98\n",
      "Episode length: 617.05 +/- 113.99\n",
      "Eval num_timesteps=4250000, episode_reward=-0.34 +/- 2.84\n",
      "Episode length: 633.73 +/- 110.19\n",
      "Eval num_timesteps=4500000, episode_reward=0.23 +/- 2.68\n",
      "Episode length: 643.02 +/- 95.91\n",
      "Eval num_timesteps=4750000, episode_reward=0.29 +/- 2.79\n",
      "Episode length: 631.45 +/- 115.44\n",
      "Eval num_timesteps=5000000, episode_reward=0.33 +/- 2.75\n",
      "Episode length: 650.20 +/- 112.04\n",
      "Eval num_timesteps=5250000, episode_reward=0.11 +/- 2.93\n",
      "Episode length: 623.22 +/- 116.26\n",
      "Eval num_timesteps=5500000, episode_reward=0.27 +/- 2.68\n",
      "Episode length: 630.26 +/- 111.38\n",
      "Eval num_timesteps=5750000, episode_reward=0.73 +/- 2.67\n",
      "Episode length: 630.93 +/- 108.34\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000000, episode_reward=0.17 +/- 2.47\n",
      "Episode length: 653.88 +/- 107.83\n",
      "Eval num_timesteps=6250000, episode_reward=-0.20 +/- 2.68\n",
      "Episode length: 654.01 +/- 110.81\n",
      "Eval num_timesteps=6500000, episode_reward=-0.03 +/- 2.84\n",
      "Episode length: 611.88 +/- 109.62\n",
      "Eval num_timesteps=6750000, episode_reward=0.74 +/- 2.77\n",
      "Episode length: 660.13 +/- 124.07\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000000, episode_reward=-0.09 +/- 2.97\n",
      "Episode length: 616.21 +/- 111.66\n",
      "Eval num_timesteps=7250000, episode_reward=0.32 +/- 2.70\n",
      "Episode length: 633.12 +/- 112.23\n",
      "Eval num_timesteps=7500000, episode_reward=0.21 +/- 2.63\n",
      "Episode length: 667.44 +/- 113.38\n",
      "Eval num_timesteps=7750000, episode_reward=1.60 +/- 2.45\n",
      "Episode length: 678.58 +/- 127.62\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000000, episode_reward=0.28 +/- 2.55\n",
      "Episode length: 645.93 +/- 96.34\n",
      "Eval num_timesteps=8250000, episode_reward=-0.73 +/- 2.49\n",
      "Episode length: 660.91 +/- 117.67\n",
      "Eval num_timesteps=8500000, episode_reward=0.24 +/- 2.52\n",
      "Episode length: 651.70 +/- 104.15\n",
      "Eval num_timesteps=8750000, episode_reward=0.28 +/- 2.83\n",
      "Episode length: 618.17 +/- 110.55\n",
      "Eval num_timesteps=9000000, episode_reward=0.13 +/- 2.75\n",
      "Episode length: 630.72 +/- 118.46\n",
      "Eval num_timesteps=9250000, episode_reward=-0.12 +/- 2.77\n",
      "Episode length: 620.40 +/- 103.71\n",
      "Eval num_timesteps=9500000, episode_reward=-0.06 +/- 2.65\n",
      "Episode length: 652.23 +/- 112.66\n",
      "Eval num_timesteps=9750000, episode_reward=0.08 +/- 2.57\n",
      "Episode length: 672.91 +/- 120.56\n",
      "Eval num_timesteps=10000000, episode_reward=-0.27 +/- 2.76\n",
      "Episode length: 642.74 +/- 120.23\n",
      "Eval num_timesteps=10250000, episode_reward=-0.45 +/- 2.63\n",
      "Episode length: 644.74 +/- 114.88\n",
      "Eval num_timesteps=10500000, episode_reward=0.46 +/- 2.76\n",
      "Episode length: 619.03 +/- 100.56\n",
      "Eval num_timesteps=10750000, episode_reward=0.46 +/- 2.70\n",
      "Episode length: 644.80 +/- 112.05\n",
      "Eval num_timesteps=11000000, episode_reward=0.87 +/- 2.82\n",
      "Episode length: 666.11 +/- 126.88\n",
      "Eval num_timesteps=11250000, episode_reward=0.04 +/- 2.86\n",
      "Episode length: 668.07 +/- 120.81\n",
      "Eval num_timesteps=11500000, episode_reward=-0.45 +/- 2.77\n",
      "Episode length: 664.43 +/- 114.13\n",
      "Eval num_timesteps=11750000, episode_reward=0.28 +/- 2.79\n",
      "Episode length: 621.67 +/- 102.42\n",
      "Eval num_timesteps=12000000, episode_reward=0.10 +/- 2.87\n",
      "Episode length: 645.14 +/- 127.52\n",
      "Eval num_timesteps=12250000, episode_reward=0.01 +/- 2.57\n",
      "Episode length: 643.12 +/- 102.09\n",
      "Eval num_timesteps=12500000, episode_reward=0.18 +/- 2.81\n",
      "Episode length: 631.19 +/- 108.59\n",
      "Eval num_timesteps=12750000, episode_reward=0.24 +/- 2.69\n",
      "Episode length: 677.37 +/- 131.06\n",
      "Eval num_timesteps=13000000, episode_reward=0.38 +/- 2.64\n",
      "Episode length: 660.82 +/- 112.90\n",
      "Eval num_timesteps=13250000, episode_reward=0.08 +/- 2.83\n",
      "Episode length: 663.49 +/- 123.60\n",
      "Eval num_timesteps=13500000, episode_reward=0.67 +/- 2.46\n",
      "Episode length: 699.47 +/- 126.76\n",
      "Eval num_timesteps=13750000, episode_reward=0.71 +/- 2.86\n",
      "Episode length: 654.89 +/- 131.93\n",
      "Eval num_timesteps=14000000, episode_reward=0.30 +/- 2.79\n",
      "Episode length: 666.63 +/- 117.46\n",
      "Eval num_timesteps=14250000, episode_reward=0.34 +/- 2.79\n",
      "Episode length: 618.72 +/- 104.03\n",
      "Eval num_timesteps=14500000, episode_reward=-0.06 +/- 2.69\n",
      "Episode length: 637.24 +/- 98.86\n",
      "Eval num_timesteps=14750000, episode_reward=-0.03 +/- 2.78\n",
      "Episode length: 666.68 +/- 131.92\n",
      "Eval num_timesteps=15000000, episode_reward=-0.02 +/- 2.71\n",
      "Episode length: 682.20 +/- 121.94\n",
      "Eval num_timesteps=15250000, episode_reward=0.82 +/- 2.80\n",
      "Episode length: 650.31 +/- 124.78\n",
      "Eval num_timesteps=15500000, episode_reward=0.16 +/- 2.88\n",
      "Episode length: 663.40 +/- 121.35\n",
      "Eval num_timesteps=15750000, episode_reward=0.80 +/- 2.86\n",
      "Episode length: 655.45 +/- 110.14\n",
      "Eval num_timesteps=16000000, episode_reward=0.88 +/- 2.71\n",
      "Episode length: 675.83 +/- 130.28\n",
      "Eval num_timesteps=16250000, episode_reward=0.15 +/- 2.76\n",
      "Episode length: 680.27 +/- 123.21\n",
      "Eval num_timesteps=16500000, episode_reward=0.61 +/- 2.67\n",
      "Episode length: 668.54 +/- 119.56\n",
      "Eval num_timesteps=16750000, episode_reward=0.07 +/- 2.67\n",
      "Episode length: 690.00 +/- 124.17\n",
      "Eval num_timesteps=17000000, episode_reward=0.14 +/- 2.87\n",
      "Episode length: 684.87 +/- 137.15\n",
      "Eval num_timesteps=17250000, episode_reward=-0.22 +/- 2.78\n",
      "Episode length: 629.67 +/- 114.05\n",
      "Eval num_timesteps=17500000, episode_reward=0.44 +/- 2.74\n",
      "Episode length: 647.73 +/- 121.29\n",
      "Eval num_timesteps=17750000, episode_reward=0.42 +/- 2.63\n",
      "Episode length: 665.43 +/- 105.54\n",
      "Eval num_timesteps=18000000, episode_reward=-0.41 +/- 2.64\n",
      "Episode length: 702.43 +/- 115.11\n",
      "Eval num_timesteps=18250000, episode_reward=0.07 +/- 2.79\n",
      "Episode length: 676.60 +/- 110.14\n",
      "Eval num_timesteps=18500000, episode_reward=0.48 +/- 2.67\n",
      "Episode length: 635.67 +/- 110.79\n",
      "Eval num_timesteps=18750000, episode_reward=0.43 +/- 2.80\n",
      "Episode length: 673.47 +/- 127.02\n",
      "Eval num_timesteps=19000000, episode_reward=0.28 +/- 2.63\n",
      "Episode length: 699.86 +/- 118.72\n",
      "Eval num_timesteps=19250000, episode_reward=0.03 +/- 2.70\n",
      "Episode length: 688.60 +/- 121.13\n",
      "Eval num_timesteps=19500000, episode_reward=0.13 +/- 2.93\n",
      "Episode length: 648.92 +/- 117.77\n",
      "Eval num_timesteps=19750000, episode_reward=-0.45 +/- 2.76\n",
      "Episode length: 691.15 +/- 109.76\n",
      "Eval num_timesteps=20000000, episode_reward=0.19 +/- 2.72\n",
      "Episode length: 721.00 +/- 135.60\n",
      "Eval num_timesteps=20250000, episode_reward=0.93 +/- 2.53\n",
      "Episode length: 700.88 +/- 134.33\n",
      "Eval num_timesteps=20500000, episode_reward=0.46 +/- 2.79\n",
      "Episode length: 694.89 +/- 138.89\n",
      "Eval num_timesteps=20750000, episode_reward=0.37 +/- 2.68\n",
      "Episode length: 717.14 +/- 125.01\n",
      "Eval num_timesteps=21000000, episode_reward=1.05 +/- 2.66\n",
      "Episode length: 687.16 +/- 137.26\n",
      "Eval num_timesteps=21250000, episode_reward=0.17 +/- 2.48\n",
      "Episode length: 710.32 +/- 126.86\n",
      "Eval num_timesteps=21500000, episode_reward=0.40 +/- 2.45\n",
      "Episode length: 742.43 +/- 120.57\n",
      "Eval num_timesteps=21750000, episode_reward=0.80 +/- 2.48\n",
      "Episode length: 726.25 +/- 127.08\n",
      "Eval num_timesteps=22000000, episode_reward=0.66 +/- 2.49\n",
      "Episode length: 692.71 +/- 115.37\n",
      "Eval num_timesteps=22250000, episode_reward=-0.17 +/- 2.65\n",
      "Episode length: 696.18 +/- 119.62\n",
      "Eval num_timesteps=22500000, episode_reward=0.43 +/- 2.83\n",
      "Episode length: 688.67 +/- 135.98\n",
      "Eval num_timesteps=22750000, episode_reward=0.73 +/- 2.68\n",
      "Episode length: 689.92 +/- 131.64\n",
      "Eval num_timesteps=23000000, episode_reward=0.44 +/- 2.94\n",
      "Episode length: 685.34 +/- 132.00\n",
      "Eval num_timesteps=23250000, episode_reward=0.87 +/- 2.69\n",
      "Episode length: 680.81 +/- 130.40\n",
      "Eval num_timesteps=23500000, episode_reward=0.08 +/- 2.75\n",
      "Episode length: 675.30 +/- 119.63\n",
      "Eval num_timesteps=23750000, episode_reward=0.03 +/- 2.66\n",
      "Episode length: 698.57 +/- 120.52\n",
      "Eval num_timesteps=24000000, episode_reward=0.59 +/- 2.56\n",
      "Episode length: 702.71 +/- 132.57\n",
      "Eval num_timesteps=24250000, episode_reward=0.02 +/- 2.79\n",
      "Episode length: 683.67 +/- 124.23\n",
      "Eval num_timesteps=24500000, episode_reward=0.10 +/- 2.61\n",
      "Episode length: 677.58 +/- 109.90\n",
      "Eval num_timesteps=24750000, episode_reward=0.31 +/- 2.69\n",
      "Episode length: 716.19 +/- 144.61\n",
      "Eval num_timesteps=25000000, episode_reward=-0.04 +/- 2.77\n",
      "Episode length: 675.20 +/- 104.42\n",
      "Eval num_timesteps=25250000, episode_reward=0.23 +/- 2.75\n",
      "Episode length: 726.18 +/- 137.09\n",
      "Eval num_timesteps=25500000, episode_reward=-0.02 +/- 2.84\n",
      "Episode length: 674.70 +/- 129.74\n",
      "Eval num_timesteps=25750000, episode_reward=-0.04 +/- 2.66\n",
      "Episode length: 724.57 +/- 124.24\n",
      "Eval num_timesteps=26000000, episode_reward=-0.16 +/- 2.78\n",
      "Episode length: 694.13 +/- 131.09\n",
      "Eval num_timesteps=26250000, episode_reward=0.46 +/- 2.68\n",
      "Episode length: 694.72 +/- 118.73\n",
      "Eval num_timesteps=26500000, episode_reward=-0.48 +/- 2.84\n",
      "Episode length: 716.94 +/- 140.30\n",
      "Eval num_timesteps=26750000, episode_reward=0.61 +/- 2.66\n",
      "Episode length: 690.93 +/- 132.62\n",
      "Eval num_timesteps=27000000, episode_reward=0.31 +/- 2.56\n",
      "Episode length: 762.31 +/- 125.86\n",
      "Eval num_timesteps=27250000, episode_reward=0.88 +/- 2.57\n",
      "Episode length: 748.04 +/- 147.42\n",
      "Eval num_timesteps=27500000, episode_reward=0.18 +/- 2.61\n",
      "Episode length: 732.19 +/- 131.23\n",
      "Eval num_timesteps=27750000, episode_reward=0.40 +/- 2.72\n",
      "Episode length: 697.54 +/- 124.98\n",
      "Eval num_timesteps=28000000, episode_reward=0.37 +/- 2.42\n",
      "Episode length: 780.07 +/- 135.84\n",
      "Eval num_timesteps=28250000, episode_reward=0.85 +/- 2.75\n",
      "Episode length: 729.25 +/- 143.31\n",
      "Eval num_timesteps=28500000, episode_reward=0.61 +/- 2.71\n",
      "Episode length: 714.11 +/- 139.32\n",
      "Eval num_timesteps=28750000, episode_reward=0.96 +/- 2.66\n",
      "Episode length: 712.31 +/- 139.35\n",
      "Eval num_timesteps=29000000, episode_reward=-0.08 +/- 2.74\n",
      "Episode length: 673.33 +/- 123.15\n",
      "Eval num_timesteps=29250000, episode_reward=0.70 +/- 2.67\n",
      "Episode length: 744.18 +/- 155.86\n",
      "Eval num_timesteps=29500000, episode_reward=0.36 +/- 2.59\n",
      "Episode length: 765.98 +/- 124.27\n",
      "Eval num_timesteps=29750000, episode_reward=0.16 +/- 2.74\n",
      "Episode length: 709.55 +/- 127.07\n",
      "Eval num_timesteps=30000000, episode_reward=0.17 +/- 2.87\n",
      "Episode length: 716.26 +/- 138.91\n",
      "Eval num_timesteps=30250000, episode_reward=0.27 +/- 2.74\n",
      "Episode length: 691.13 +/- 121.76\n",
      "Eval num_timesteps=30500000, episode_reward=1.46 +/- 2.50\n",
      "Episode length: 724.81 +/- 145.53\n",
      "Eval num_timesteps=30750000, episode_reward=0.31 +/- 2.70\n",
      "Episode length: 723.94 +/- 135.12\n",
      "Eval num_timesteps=31000000, episode_reward=0.99 +/- 2.71\n",
      "Episode length: 735.85 +/- 151.25\n",
      "Eval num_timesteps=31250000, episode_reward=-0.07 +/- 2.69\n",
      "Episode length: 750.55 +/- 132.78\n",
      "Eval num_timesteps=31500000, episode_reward=1.06 +/- 2.81\n",
      "Episode length: 731.03 +/- 159.45\n",
      "Eval num_timesteps=31750000, episode_reward=1.22 +/- 2.63\n",
      "Episode length: 778.75 +/- 159.79\n",
      "Eval num_timesteps=32000000, episode_reward=0.96 +/- 2.73\n",
      "Episode length: 729.37 +/- 139.86\n",
      "Eval num_timesteps=32250000, episode_reward=1.84 +/- 2.55\n",
      "Episode length: 735.51 +/- 151.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32500000, episode_reward=1.96 +/- 2.24\n",
      "Episode length: 715.25 +/- 150.51\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32750000, episode_reward=0.85 +/- 2.79\n",
      "Episode length: 727.70 +/- 155.74\n",
      "Eval num_timesteps=33000000, episode_reward=0.72 +/- 2.77\n",
      "Episode length: 835.37 +/- 187.65\n",
      "Eval num_timesteps=33250000, episode_reward=2.66 +/- 1.92\n",
      "Episode length: 713.53 +/- 150.19\n",
      "New best mean reward!\n",
      "Eval num_timesteps=33500000, episode_reward=0.89 +/- 2.75\n",
      "Episode length: 809.51 +/- 182.60\n",
      "Eval num_timesteps=33750000, episode_reward=0.21 +/- 2.70\n",
      "Episode length: 873.57 +/- 172.01\n",
      "Eval num_timesteps=34000000, episode_reward=1.65 +/- 2.61\n",
      "Episode length: 761.54 +/- 167.85\n",
      "Eval num_timesteps=34250000, episode_reward=0.56 +/- 2.88\n",
      "Episode length: 858.77 +/- 194.76\n",
      "Eval num_timesteps=34500000, episode_reward=-0.04 +/- 2.63\n",
      "Episode length: 870.85 +/- 155.11\n",
      "Eval num_timesteps=34750000, episode_reward=3.02 +/- 1.89\n",
      "Episode length: 884.89 +/- 242.32\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000000, episode_reward=1.74 +/- 2.43\n",
      "Episode length: 927.06 +/- 243.40\n",
      "Eval num_timesteps=35250000, episode_reward=0.12 +/- 3.03\n",
      "Episode length: 788.12 +/- 164.81\n",
      "Eval num_timesteps=35500000, episode_reward=3.28 +/- 1.50\n",
      "Episode length: 807.05 +/- 227.93\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35750000, episode_reward=2.25 +/- 2.20\n",
      "Episode length: 972.82 +/- 309.00\n",
      "Eval num_timesteps=36000000, episode_reward=0.30 +/- 2.70\n",
      "Episode length: 811.80 +/- 163.51\n",
      "Eval num_timesteps=36250000, episode_reward=-0.64 +/- 2.65\n",
      "Episode length: 724.86 +/- 134.08\n",
      "Eval num_timesteps=36500000, episode_reward=1.44 +/- 2.50\n",
      "Episode length: 989.16 +/- 270.21\n",
      "Eval num_timesteps=36750000, episode_reward=2.49 +/- 1.95\n",
      "Episode length: 930.11 +/- 295.74\n",
      "Eval num_timesteps=37000000, episode_reward=0.90 +/- 2.86\n",
      "Episode length: 1010.13 +/- 263.14\n",
      "Eval num_timesteps=37250000, episode_reward=3.22 +/- 1.78\n",
      "Episode length: 1197.57 +/- 433.19\n",
      "Eval num_timesteps=37500000, episode_reward=1.63 +/- 2.54\n",
      "Episode length: 860.28 +/- 217.80\n",
      "Eval num_timesteps=37750000, episode_reward=0.99 +/- 2.75\n",
      "Episode length: 888.64 +/- 218.20\n",
      "Eval num_timesteps=38000000, episode_reward=3.27 +/- 1.74\n",
      "Episode length: 827.94 +/- 245.63\n",
      "Eval num_timesteps=38250000, episode_reward=2.72 +/- 1.97\n",
      "Episode length: 1228.29 +/- 524.02\n",
      "Eval num_timesteps=38500000, episode_reward=1.42 +/- 2.48\n",
      "Episode length: 1796.04 +/- 660.19\n",
      "Eval num_timesteps=38750000, episode_reward=-0.27 +/- 2.88\n",
      "Episode length: 976.82 +/- 235.18\n",
      "Eval num_timesteps=39000000, episode_reward=1.60 +/- 2.58\n",
      "Episode length: 977.82 +/- 297.33\n",
      "Eval num_timesteps=39250000, episode_reward=3.14 +/- 1.73\n",
      "Episode length: 760.18 +/- 178.53\n",
      "Eval num_timesteps=39500000, episode_reward=1.13 +/- 2.80\n",
      "Episode length: 1105.57 +/- 353.80\n",
      "Eval num_timesteps=39750000, episode_reward=0.79 +/- 2.66\n",
      "Episode length: 1234.95 +/- 364.83\n",
      "Eval num_timesteps=40000000, episode_reward=2.85 +/- 1.97\n",
      "Episode length: 942.64 +/- 303.71\n",
      "Eval num_timesteps=40250000, episode_reward=4.53 +/- 0.68\n",
      "Episode length: 744.34 +/- 211.26\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40500000, episode_reward=1.80 +/- 2.82\n",
      "Episode length: 926.64 +/- 276.22\n",
      "Eval num_timesteps=40750000, episode_reward=4.32 +/- 0.84\n",
      "Episode length: 930.36 +/- 248.07\n",
      "Eval num_timesteps=41000000, episode_reward=0.03 +/- 2.97\n",
      "Episode length: 1050.19 +/- 320.78\n",
      "Eval num_timesteps=41250000, episode_reward=-0.50 +/- 2.99\n",
      "Episode length: 1142.78 +/- 411.17\n",
      "Eval num_timesteps=41500000, episode_reward=2.95 +/- 2.02\n",
      "Episode length: 1416.95 +/- 545.55\n",
      "Eval num_timesteps=41750000, episode_reward=3.29 +/- 1.42\n",
      "Episode length: 973.11 +/- 279.98\n",
      "Eval num_timesteps=42000000, episode_reward=1.36 +/- 2.84\n",
      "Episode length: 1052.42 +/- 287.71\n",
      "Eval num_timesteps=42250000, episode_reward=4.14 +/- 1.03\n",
      "Episode length: 973.34 +/- 316.51\n",
      "Eval num_timesteps=42500000, episode_reward=2.63 +/- 2.14\n",
      "Episode length: 862.93 +/- 252.48\n",
      "Eval num_timesteps=42750000, episode_reward=3.92 +/- 1.05\n",
      "Episode length: 1057.37 +/- 334.34\n",
      "Eval num_timesteps=43000000, episode_reward=3.96 +/- 1.20\n",
      "Episode length: 1142.86 +/- 449.26\n",
      "Eval num_timesteps=43250000, episode_reward=3.25 +/- 1.32\n",
      "Episode length: 1064.48 +/- 289.11\n",
      "Eval num_timesteps=43500000, episode_reward=2.12 +/- 2.43\n",
      "Episode length: 1058.27 +/- 329.05\n",
      "Eval num_timesteps=43750000, episode_reward=1.99 +/- 2.35\n",
      "Episode length: 1134.34 +/- 326.64\n",
      "Eval num_timesteps=44000000, episode_reward=4.54 +/- 0.68\n",
      "Episode length: 990.48 +/- 313.81\n",
      "New best mean reward!\n",
      "Eval num_timesteps=44250000, episode_reward=4.27 +/- 1.03\n",
      "Episode length: 1074.87 +/- 345.46\n",
      "Eval num_timesteps=44500000, episode_reward=2.08 +/- 2.19\n",
      "Episode length: 1102.15 +/- 330.91\n",
      "Eval num_timesteps=44750000, episode_reward=2.90 +/- 1.81\n",
      "Episode length: 1241.40 +/- 455.83\n",
      "Eval num_timesteps=45000000, episode_reward=3.37 +/- 1.55\n",
      "Episode length: 1157.72 +/- 401.03\n",
      "Eval num_timesteps=45250000, episode_reward=1.79 +/- 2.62\n",
      "Episode length: 1162.26 +/- 409.35\n",
      "Eval num_timesteps=45500000, episode_reward=3.18 +/- 1.72\n",
      "Episode length: 1434.90 +/- 482.79\n",
      "Eval num_timesteps=45750000, episode_reward=4.56 +/- 0.99\n",
      "Episode length: 1255.13 +/- 513.11\n",
      "New best mean reward!\n",
      "Eval num_timesteps=46000000, episode_reward=2.80 +/- 1.89\n",
      "Episode length: 1974.61 +/- 718.32\n",
      "Eval num_timesteps=46250000, episode_reward=1.81 +/- 2.15\n",
      "Episode length: 1350.47 +/- 512.41\n",
      "Eval num_timesteps=46500000, episode_reward=4.24 +/- 0.92\n",
      "Episode length: 1000.42 +/- 323.23\n",
      "Eval num_timesteps=46750000, episode_reward=4.00 +/- 1.11\n",
      "Episode length: 1927.16 +/- 680.68\n",
      "Eval num_timesteps=47000000, episode_reward=4.32 +/- 0.89\n",
      "Episode length: 929.85 +/- 280.74\n",
      "Eval num_timesteps=47250000, episode_reward=4.36 +/- 0.74\n",
      "Episode length: 811.81 +/- 210.83\n",
      "Eval num_timesteps=47500000, episode_reward=1.59 +/- 2.45\n",
      "Episode length: 1285.94 +/- 430.25\n",
      "Eval num_timesteps=47750000, episode_reward=3.45 +/- 1.45\n",
      "Episode length: 1107.23 +/- 371.80\n",
      "Eval num_timesteps=48000000, episode_reward=4.17 +/- 1.15\n",
      "Episode length: 844.59 +/- 224.77\n",
      "Eval num_timesteps=48250000, episode_reward=4.27 +/- 0.98\n",
      "Episode length: 1096.17 +/- 441.69\n",
      "Eval num_timesteps=48500000, episode_reward=3.85 +/- 1.31\n",
      "Episode length: 907.36 +/- 258.13\n",
      "Eval num_timesteps=48750000, episode_reward=4.34 +/- 0.85\n",
      "Episode length: 825.57 +/- 200.49\n",
      "Eval num_timesteps=49000000, episode_reward=4.60 +/- 0.71\n",
      "Episode length: 849.72 +/- 263.47\n",
      "New best mean reward!\n",
      "Eval num_timesteps=49250000, episode_reward=4.14 +/- 0.93\n",
      "Episode length: 1132.00 +/- 401.04\n",
      "Eval num_timesteps=49500000, episode_reward=4.00 +/- 1.08\n",
      "Episode length: 988.85 +/- 371.69\n",
      "Eval num_timesteps=49750000, episode_reward=4.16 +/- 1.01\n",
      "Episode length: 1282.74 +/- 522.70\n",
      "Eval num_timesteps=50000000, episode_reward=4.57 +/- 0.75\n",
      "Episode length: 925.44 +/- 295.55\n"
     ]
    }
   ],
   "source": [
    "# wrapper over the normal single player env, but loads the best self play model\n",
    "class SlimeVolleySelfPlayEnv(slimevolleygym.SlimeVolleyEnv):\n",
    "  \n",
    "  def __init__(self):\n",
    "    super(SlimeVolleySelfPlayEnv, self).__init__()\n",
    "    self.policy = self\n",
    "    self.best_model = None\n",
    "    self.best_model_filename = None\n",
    "\n",
    "  def predict(self, obs): # the policy\n",
    "    if self.best_model is None:\n",
    "      return self.action_space.sample() # return a random action\n",
    "    else:\n",
    "      action, _ = self.best_model.predict(obs)\n",
    "      return action\n",
    "\n",
    "  # load model if it's there\n",
    "  def reset(self):\n",
    "    modellist = [f for f in os.listdir(LOGDIR) if f.startswith(\"history\")]\n",
    "    modellist.sort()\n",
    "    if len(modellist) > 0:\n",
    "      filename = os.path.join(LOGDIR, modellist[-1]) # the latest generation model\n",
    "      if filename != self.best_model_filename:\n",
    "        self.best_model_filename = filename\n",
    "        if self.best_model is not None:\n",
    "          del self.best_model\n",
    "        # Note: Due to a bug in sb3, we had to modify the load_from_zip_file() function in common/save_utils.py of the sb3 library\n",
    "        # Set weights_only=False in the load line of the library code\n",
    "        self.best_model = A2C.load(filename, env=self, weights_only=False)\n",
    "    return super(SlimeVolleySelfPlayEnv, self).reset()\n",
    "\n",
    "# hacked it to only save new version of best model if beats prev self by BEST_THRESHOLD score\n",
    "# after saving model, resets the best score to be BEST_THRESHOLD\n",
    "class SelfPlayCallback(EvalCallback):\n",
    "\n",
    "  def __init__(self, vec_env_selfplay, best_model_save_path, log_path, eval_freq, n_eval_episodes_selfplay, n_eval_episodes_baseline, n_cpu, model):\n",
    "\n",
    "    # Call the superclass constructor with all arguments except the model\n",
    "    super(SelfPlayCallback, self).__init__(vec_env_selfplay, \n",
    "                                           best_model_save_path=best_model_save_path, \n",
    "                                           log_path=log_path, \n",
    "                                           eval_freq=eval_freq, \n",
    "                                           n_eval_episodes=n_eval_episodes_selfplay)\n",
    "    self.generation = 0\n",
    "    self.stored_model = model # Store a reference to the model being trained in order to save across generations\n",
    "    self.n_eval_episodes_baseline = n_eval_episodes_baseline\n",
    "    self.baseline = BaselinePolicy()\n",
    "    self.n_cpu = n_cpu\n",
    "\n",
    "    # Make the regular environment with the opponent being the baseline instead of selfplay for this callback\n",
    "    self.vec_env_baseline = make_vec_env(slimevolleygym.SlimeVolleyEnv, n_envs=n_cpu, seed=SEED)\n",
    "\n",
    "    # Create a summarywriter at the logdir\n",
    "    self.writer = SummaryWriter(log_dir=f\"{LOGDIR}\")\n",
    "      \n",
    "  def _on_step(self) -> bool:\n",
    "\n",
    "    # Updates self.last_mean_reward to be the mean reward of the selfplay evaluation (For n_eval_episodes)\n",
    "    result = super(SelfPlayCallback, self)._on_step()\n",
    "\n",
    "    # Check if it's time to run the evaluation callback for the baseline\n",
    "    mean_baseline_reward = -100\n",
    "    std_baseline_reward = -100\n",
    "    if result and self.n_calls > 0 and self.n_calls % self.eval_freq == 0:\n",
    "        \n",
    "        # Evaluate the model in the vectorized environment\n",
    "        mean_baseline_reward, std_baseline_reward = evaluate_policy(self.stored_model, self.vec_env_baseline, n_eval_episodes=self.n_eval_episodes_baseline)\n",
    "\n",
    "        # Log the mean and std reward\n",
    "        self.writer.add_scalar(\"Average baseline test return - Training step\", mean_baseline_reward, self.n_calls * self.n_cpu)\n",
    "        self.writer.add_scalar(\"Baseline test return standard deviation - Training step\", std_baseline_reward, self.n_calls * self.n_cpu)\n",
    "\n",
    "        # Only update the generation if we perform better than the threshold set\n",
    "        if mean_baseline_reward != -100 and self.last_mean_reward > BEST_THRESHOLD:\n",
    "          self.generation += 1\n",
    "          save_model_name = os.path.join(LOGDIR, \"history_\"+str(self.generation).zfill(8)+\".zip\")\n",
    "          self.stored_model.save(save_model_name)\n",
    "           \n",
    "          # Log the mean and std baseline reward as a function of the generation number\n",
    "          self.writer.add_scalar(\"Average baseline test return - Generation\", mean_baseline_reward, self.generation)\n",
    "          self.writer.add_scalar(\"Baseline test return standard deviation - Generation\", std_baseline_reward, self.generation)\n",
    "\n",
    "    # Return the success / failure flag of the parent _on_step call\n",
    "    return result\n",
    "    \n",
    "def train():\n",
    "\n",
    "  vec_env = make_vec_env(SlimeVolleySelfPlayEnv, n_envs=n_cpu, seed=SEED)\n",
    "\n",
    "  model = A2C(\"MlpPolicy\", \n",
    "              vec_env, \n",
    "              learning_rate=learning_rate, \n",
    "              n_steps=n_steps, \n",
    "              gamma=gamma, \n",
    "              gae_lambda=gae_lambda, \n",
    "              ent_coef=ent_coef, \n",
    "              vf_coef=vf_coef, \n",
    "              max_grad_norm=max_grad_norm,\n",
    "              rms_prop_eps=rms_prop_eps, \n",
    "              use_rms_prop=use_rms_prop, \n",
    "              use_sde=use_sde, \n",
    "              sde_sample_freq=sde_sample_freq, \n",
    "              rollout_buffer_class=rollout_buffer_class, \n",
    "              rollout_buffer_kwargs=rollout_buffer_kwargs,\n",
    "              normalize_advantage=normalize_advantage, \n",
    "              stats_window_size=stats_window_size, \n",
    "              tensorboard_log=LOGDIR, \n",
    "              policy_kwargs=policy_kwargs, \n",
    "              verbose=verbose, \n",
    "              seed=SEED, \n",
    "              device=device,\n",
    "              _init_setup_model=_init_setup_model)\n",
    "\n",
    "  selfplay_eval_callback = SelfPlayCallback(vec_env_selfplay=vec_env,\n",
    "                                              best_model_save_path=LOGDIR,\n",
    "                                              log_path=LOGDIR,\n",
    "                                              eval_freq=EVAL_FREQ,\n",
    "                                              n_eval_episodes_selfplay=EVAL_EPISODES_SELFPLAY,\n",
    "                                              model=model,\n",
    "                                              n_eval_episodes_baseline=EVAL_EPISODES_BASELINE,\n",
    "                                              n_cpu=n_cpu)\n",
    "\n",
    "  model.learn(total_timesteps=NUM_TIMESTEPS, callback=selfplay_eval_callback)\n",
    "  model.save(os.path.join(LOGDIR, \"final_model\"))\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "  train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
