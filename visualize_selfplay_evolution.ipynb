{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.PPO.PPO_Agent import PPO_Agent\n",
    "import torch\n",
    "import slimevolleygym\n",
    "from utils import convert_to_vector, convert_to_value\n",
    "import types\n",
    "import slimevolleygym\n",
    "import slimevolleygym.mlp as mlp\n",
    "from slimevolleygym.mlp import Model\n",
    "import json\n",
    "from slimevolleygym import BaselinePolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used:  mps\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "# Print the device as a check\n",
    "print(\"Device used: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize evolution of PPO self-play generations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the self-play training of PPO with the entropy coefficient set to 0.1 to visualize the evolution of generations as the self-play training progresses for PPO. We chose this setting because, among the two configurations tested, it demonstrated the greatest evolution, reaching 29 generations as opposed to only 16 when the entropy coefficient was set to 0.\n",
    "\n",
    "We analyze four distinct generations, utilizing the checkpoints that are available. We determine the specific generations by reviewing the logs displayed on Tensorboard, where we have charted the generation numbers against training steps. It is crucial to mention that we consistently save both the original training agent and its duplicate at every step during the training process. Consequently, we choose checkpoints that fall between generations X and Y, and we load agent 2 to capture the state of the agent at generation X. The generations are as follows:\n",
    "- First, we load the checkpoint performed at step 5,833,042, which corresponds to the 6th generation (which is from the 5,37e+6 step). Unfortunately, this checkpoint occured just before the change to the 7th generation at the 5,84e+6 step.\n",
    "- Then, we load the checkpoint performed at step 10,673,611, which corresponds to the 14th generation (which is from the 1,01e+7 step). Again, this checkpoint occured just before the change to the 15th generation at the 1,07e+7 step.\n",
    "- Third, we load the checkpoint performed at step 14,738,250, which corresponds to the 22nd generation (which is from the 1,47e+7 step). Generation 23 only occurs at the 1,53e+7 step.\n",
    "- Finally, we load the checkpoint performed at step 20,001,157, which corresponds to the 29th generation (which is from the 1,95e+7 step).\n",
    "\n",
    "Thus, this code will load the geenrations 6, 14, 22 and 29, and visualize the evolution of the self-play training by playing the agents against themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"Logging/PPO-SELFPLAY/20240411-190831-lr-0.0003-entcoef-0.1\"\n",
    "# a) Load the self play PPO agent with entropy coefficient 0.1 at generation 6 and play it against itself\n",
    "# agent1 = PPO_Agent(12, 6, DEVICE, mlp_layers=[64, 64])\n",
    "# agent1.load_models(dir, 2,  5833042)\n",
    "# agent2 = PPO_Agent(12, 6, DEVICE, mlp_layers=[64, 64])\n",
    "# agent2.load_models(dir, 2,  5833042)\n",
    "\n",
    "# b) Load the self play PPO agent with entropy coefficient 0.1 at generation 14 and play it against itself\n",
    "# agent1 = PPO_Agent(12, 6, DEVICE, mlp_layers=[64, 64])\n",
    "# agent1.load_models(dir, 2,  10673611)\n",
    "# agent2 = PPO_Agent(12, 6, DEVICE, mlp_layers=[64, 64])\n",
    "# agent2.load_models(dir, 2,  10673611)\n",
    "\n",
    "# c) Load the self play PPO agent with entropy coefficient 0.1 at generation 22 and play it against itself\n",
    "# agent1 = PPO_Agent(12, 6, DEVICE, mlp_layers=[64, 64])\n",
    "# agent1.load_models(dir, 2,  14738250)\n",
    "# agent2 = PPO_Agent(12, 6, DEVICE, mlp_layers=[64, 64])\n",
    "# agent2.load_models(dir, 2,  14738250)\n",
    "\n",
    "# d) Load the self play PPO agent with entropy coefficient 0.1 at generation 29 and play it against itself\n",
    "# agent1 = PPO_Agent(12, 6, DEVICE, mlp_layers=[64, 64])\n",
    "# agent1.load_models(dir, 2,  20001157)\n",
    "# agent2 = PPO_Agent(12, 6, DEVICE, mlp_layers=[64, 64])\n",
    "# agent2.load_models(dir, 2,  20001157)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 17:35:23.885 Python[79927:13394176] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/dk/m9d4fkln0d947z3d_fc1m8000000gn/T/org.python.python.savedState\n"
     ]
    }
   ],
   "source": [
    "# Set the agents to evaluation mode\n",
    "agent1.evaluation_mode()\n",
    "agent2.evaluation_mode()\n",
    "\n",
    "# Run num_eval_episodes episodes and calculate the total return\n",
    "total_return = 0\n",
    "for _ in range(1):\n",
    "\n",
    "    state1 = env.reset()\n",
    "    state2 = state1\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        # Render the environment\n",
    "        env.render(mode='human')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Select the actions for each agent\n",
    "            action1, _ = agent1.select_action(state1, greedy=True)\n",
    "            action2, _ = agent2.select_action(state2, greedy=True)\n",
    "        \n",
    "        # Step the environment forward\n",
    "        next_state1, reward, done, info = env.step(convert_to_vector(action1), otherAction=convert_to_vector(action2))\n",
    "        next_state2 = info['otherObs']\n",
    "        \n",
    "        # Add the individual agents' rewards to the total returns (Since they're the same for both agents)\n",
    "        total_return += reward\n",
    "\n",
    "        # Update the states\n",
    "        state1 = next_state1\n",
    "        state2 = next_state2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
