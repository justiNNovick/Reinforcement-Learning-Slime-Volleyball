{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.PPO.PPO_Agent import PPO_Agent\n",
    "from Models.DDQN.DDQN_Agent import DDQN_Agent\n",
    "from Models.DDQN.PRB import PrioritizedReplayBuffer\n",
    "from stable_baselines3 import A2C\n",
    "import slimevolleygym.mlp as mlp\n",
    "from slimevolleygym.mlp import Model\n",
    "import torch\n",
    "import slimevolleygym\n",
    "from slimevolleygym import BaselinePolicy\n",
    "from utils import convert_to_vector, convert_to_value, convert_list_to_vectors\n",
    "import types\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "# Print the device as a check\n",
    "print(\"Device used: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all the models as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38284/2112622297.py:7: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  agent = Model(mlp.games['slimevolleylite'])\n",
      "/tmp/ipykernel_38284/2112622297.py:10: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  agent.set_model_params(d[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CURRENT SYSTEM INFO ==\n",
      "- OS: Linux-5.4.0-144-generic-x86_64-with-glibc2.27 # 161~18.04.1-Ubuntu SMP Fri Feb 10 15:55:22 UTC 2023\n",
      "- Python: 3.9.12\n",
      "- Stable-Baselines3: 2.3.0\n",
      "- PyTorch: 1.13.0+cu116\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.26.4\n",
      "- Cloudpickle: 2.2.1\n",
      "- Gymnasium: 0.29.1\n",
      "- OpenAI Gym: 0.26.2\n",
      "\n",
      "== SAVED MODEL SYSTEM INFO ==\n",
      "- OS: Linux-5.4.0-144-generic-x86_64-with-glibc2.27 # 161~18.04.1-Ubuntu SMP Fri Feb 10 15:55:22 UTC 2023\n",
      "- Python: 3.9.12\n",
      "- Stable-Baselines3: 2.3.0\n",
      "- PyTorch: 1.13.0+cu116\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.26.4\n",
      "- Cloudpickle: 2.2.1\n",
      "- Gymnasium: 0.29.1\n",
      "- OpenAI Gym: 0.26.2\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "== CURRENT SYSTEM INFO ==\n",
      "- OS: Linux-5.4.0-144-generic-x86_64-with-glibc2.27 # 161~18.04.1-Ubuntu SMP Fri Feb 10 15:55:22 UTC 2023\n",
      "- Python: 3.9.12\n",
      "- Stable-Baselines3: 2.3.0\n",
      "- PyTorch: 1.13.0+cu116\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.26.4\n",
      "- Cloudpickle: 2.2.1\n",
      "- Gymnasium: 0.29.1\n",
      "- OpenAI Gym: 0.26.2\n",
      "\n",
      "== SAVED MODEL SYSTEM INFO ==\n",
      "- OS: Linux-5.4.0-144-generic-x86_64-with-glibc2.27 # 161~18.04.1-Ubuntu SMP Fri Feb 10 15:55:22 UTC 2023\n",
      "- Python: 3.9.12\n",
      "- Stable-Baselines3: 2.3.0\n",
      "- PyTorch: 1.13.0+cu116\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.26.4\n",
      "- Cloudpickle: 2.2.1\n",
      "- Gymnasium: 0.29.1\n",
      "- OpenAI Gym: 0.26.2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/stable_baselines3/common/vec_env/patch_gym.py:95: UserWarning: You loaded a model that was trained using OpenAI Gym. We strongly recommend transitioning to Gymnasium by saving that model again.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "env = slimevolleygym.SlimeVolleyEnv()\n",
    "models = []\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "# 0- Genetic agent\n",
    "agent = Model(mlp.games['slimevolleylite'])\n",
    "with open('Logging/GENETIC-SELFPLAY/20240409-021844-numagents-128-totalnumgames-546000/game_546000') as f:\n",
    "    d = json.load(f)\n",
    "    agent.set_model_params(d[0])\n",
    "def select_action(self, state, greedy=False):\n",
    "    action = self.predict(state, mean_mode=greedy)\n",
    "    action = (action > 0).astype(int) # Anything positive means a 1, 0 or negative means a 0\n",
    "    return convert_to_value(action), None\n",
    "def evaluation_mode(self):\n",
    "    pass\n",
    "agent.select_action = types.MethodType(select_action, agent)\n",
    "agent.evaluation_mode = types.MethodType(evaluation_mode, agent)\n",
    "models.append({\n",
    "    \"name\": \"Genetic - Selfplay\",\n",
    "    \"agent\": agent\n",
    "})\n",
    "\n",
    "# 1- PPO Baseline\n",
    "agent = PPO_Agent(12, 6, DEVICE, mlp_layers=[64, 64])\n",
    "agent.load_models(\"Logging/PPO-BASELINE/20240411-150526-lr-0.0003-entcoef-0.1-mlp-64-kl-0.03\", 1, 18492436)\n",
    "models.append({\n",
    "    \"name\": \"PPO - Expert training\",\n",
    "    \"agent\": agent\n",
    "})\n",
    "\n",
    "# 2- PPO Selfplay\n",
    "agent = PPO_Agent(12, 6, DEVICE, mlp_layers=[64, 64])\n",
    "agent.load_models(\"Logging/PPO-SELFPLAY/20240410-171658-lr-0.0003-entcoef-0\", 1, 18534177)\n",
    "models.append({\n",
    "    \"name\": \"PPO - Selfplay\",\n",
    "    \"agent\": agent\n",
    "})\n",
    "\n",
    "# 3- A2C Baseline\n",
    "agent = A2C.load(\"Logging/A2C-BASELINE-LIBRARY/20240416-004821-lr-0.0007-entcoef-0.1/best_model\", env,\\\n",
    "                  print_system_info=True, custom_objects={'observation_space': env.observation_space, 'action_space': env.action_space})\n",
    "def select_action(self, state, greedy=False):\n",
    "    action, _ = self.predict(state, deterministic=greedy)\n",
    "    return convert_to_value(action), None\n",
    "def evaluation_mode(self):\n",
    "    pass\n",
    "agent.select_action = types.MethodType(select_action, agent)\n",
    "agent.evaluation_mode = types.MethodType(evaluation_mode, agent)\n",
    "models.append({\n",
    "    \"name\": \"A2C - Expert training\",\n",
    "    \"agent\": agent\n",
    "})\n",
    "\n",
    "# 4- A2C Self-play\n",
    "agent = A2C.load(\"Logging/A2C-SELFPLAY-LIBRARY/20240416-192851-lr-0.0007-entcoef-0.1/history_00000080\", env, \\\n",
    "                 print_system_info=True, custom_objects={'observation_space': env.observation_space, 'action_space': env.action_space})\n",
    "def select_action(self, state, greedy=False):\n",
    "    action, _ = self.predict(state, deterministic=greedy)\n",
    "    return convert_to_value(action), None\n",
    "def evaluation_mode(self):\n",
    "    pass\n",
    "agent.select_action = types.MethodType(select_action, agent)\n",
    "agent.evaluation_mode = types.MethodType(evaluation_mode, agent)\n",
    "models.append({\n",
    "    \"name\": \"A2C - Selfplay\",\n",
    "    \"agent\": agent\n",
    "})\n",
    "\n",
    "# 5- Baseline\n",
    "agent = BaselinePolicy()\n",
    "def select_action(self, state, greedy=False):\n",
    "    action = self.predict(state)\n",
    "    return convert_to_value(action), None\n",
    "def evaluation_mode(self):\n",
    "    pass\n",
    "agent.select_action = types.MethodType(select_action, agent)\n",
    "agent.evaluation_mode = types.MethodType(evaluation_mode, agent)\n",
    "models.append({\n",
    "    \"name\": \"Expert baseline\",\n",
    "    \"agent\": agent\n",
    "})\n",
    "\n",
    "# 6- Random agent\n",
    "agent = BaselinePolicy()\n",
    "def select_action(self, state, greedy=False):\n",
    "    action = convert_to_value(env.action_space.sample())\n",
    "    return action, None\n",
    "def evaluation_mode(self):\n",
    "    pass\n",
    "agent.select_action = types.MethodType(select_action, agent)\n",
    "agent.evaluation_mode = types.MethodType(evaluation_mode, agent)\n",
    "models.append({\n",
    "    \"name\": \"Random baseline\",\n",
    "    \"agent\": agent\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing a wrapper to evaluate the agents in parallel using sb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SlimeVolleyParallelWrapper(slimevolleygym.SlimeVolleyEnv):\n",
    "  \n",
    "    def __init__(self, opponent_agent):\n",
    "        super(SlimeVolleyParallelWrapper, self).__init__()\n",
    "        opponent_agent.evaluation_mode()\n",
    "        self.policy = opponent_agent # Store the opponent policy\n",
    "\n",
    "    # Override step to use select_action instead of predict\n",
    "    def step(self, action, otherAction=None):\n",
    "        \"\"\"\n",
    "        baseAction is only used if multiagent mode is True\n",
    "        note: although the action space is multi-binary, float vectors\n",
    "        are fine (refer to setAction() to see how they get interpreted)\n",
    "        \"\"\"\n",
    "        done = False\n",
    "        self.t += 1\n",
    "    \n",
    "        if self.otherAction is not None:\n",
    "          otherAction = self.otherAction\n",
    "          \n",
    "        if otherAction is None: # override baseline policy\n",
    "          obs = self.game.agent_left.getObservation()\n",
    "    \n",
    "          # Modification: Predict the otherAction\n",
    "          otherAction, _ = self.policy.select_action(obs, greedy=True)\n",
    "          otherAction = convert_to_vector(otherAction)\n",
    "    \n",
    "        if self.atari_mode:\n",
    "          action = self.discreteToBox(action)\n",
    "          otherAction = self.discreteToBox(otherAction)\n",
    "    \n",
    "        self.game.agent_left.setAction(otherAction)\n",
    "        self.game.agent_right.setAction(action) # external agent is agent_right\n",
    "    \n",
    "        reward = self.game.step()\n",
    "    \n",
    "        obs = self.getObs()\n",
    "    \n",
    "        if self.t >= self.t_limit:\n",
    "          done = True\n",
    "    \n",
    "        if self.game.agent_left.life <= 0 or self.game.agent_right.life <= 0:\n",
    "          done = True\n",
    "    \n",
    "        otherObs = None\n",
    "        if self.multiagent:\n",
    "          if self.from_pixels:\n",
    "            otherObs = cv2.flip(obs, 1) # horizontal flip\n",
    "          else:\n",
    "            otherObs = self.game.agent_left.getObservation()\n",
    "    \n",
    "        info = {\n",
    "          'ale.lives': self.game.agent_right.lives(),\n",
    "          'ale.otherLives': self.game.agent_left.lives(),\n",
    "          'otherObs': otherObs,\n",
    "          'state': self.game.agent_right.getObservation(),\n",
    "          'otherState': self.game.agent_left.getObservation(),\n",
    "        }\n",
    "    \n",
    "        if self.survival_bonus:\n",
    "          return obs, reward+0.01, done, info\n",
    "        return obs, reward, done, info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then modify the evaluate_policy function from the stable baselines 3 library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/evaluation.py\n",
    "# Modified to be able to run our custom agents in a parallelized way (1 environment per CPU core)\n",
    "\n",
    "import warnings\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common import type_aliases\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecEnv, VecMonitor, is_vecenv_wrapped\n",
    "\n",
    "\n",
    "def evaluate_policy(\n",
    "    model,\n",
    "    env: Union[gym.Env, VecEnv],\n",
    "    n_eval_episodes: int = 10,\n",
    "    render: bool = False,\n",
    "    callback: Optional[Callable[[Dict[str, Any], Dict[str, Any]], None]] = None,\n",
    "    reward_threshold: Optional[float] = None,\n",
    "    return_episode_rewards: bool = False,\n",
    "    warn: bool = True,\n",
    "    combination = None # Modification: For printing the progress\n",
    ") -> Union[Tuple[float, float], Tuple[List[float], List[int]]]:\n",
    "    \"\"\"\n",
    "    Runs policy for ``n_eval_episodes`` episodes and returns average reward.\n",
    "    If a vector env is passed in, this divides the episodes to evaluate onto the\n",
    "    different elements of the vector env. This static division of work is done to\n",
    "    remove bias. See https://github.com/DLR-RM/stable-baselines3/issues/402 for more\n",
    "    details and discussion.\n",
    "\n",
    "    .. note::\n",
    "        If environment has not been wrapped with ``Monitor`` wrapper, reward and\n",
    "        episode lengths are counted as it appears with ``env.step`` calls. If\n",
    "        the environment contains wrappers that modify rewards or episode lengths\n",
    "        (e.g. reward scaling, early episode reset), these will affect the evaluation\n",
    "        results as well. You can avoid this by wrapping environment with ``Monitor``\n",
    "        wrapper before anything else.\n",
    "\n",
    "    :param model: The RL agent you want to evaluate. This can be any object\n",
    "        that implements a `predict` method, such as an RL algorithm (``BaseAlgorithm``)\n",
    "        or policy (``BasePolicy``).\n",
    "    :param env: The gym environment or ``VecEnv`` environment.\n",
    "    :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "    :param deterministic: Whether to use deterministic or stochastic actions\n",
    "    :param render: Whether to render the environment or not\n",
    "    :param callback: callback function to do additional checks,\n",
    "        called after each step. Gets locals() and globals() passed as parameters.\n",
    "    :param reward_threshold: Minimum expected reward per episode,\n",
    "        this will raise an error if the performance is not met\n",
    "    :param return_episode_rewards: If True, a list of rewards and episode lengths\n",
    "        per episode will be returned instead of the mean.\n",
    "    :param warn: If True (default), warns user about lack of a Monitor wrapper in the\n",
    "        evaluation environment.\n",
    "    :return: Mean reward per episode, std of reward per episode.\n",
    "        Returns ([float], [int]) when ``return_episode_rewards`` is True, first\n",
    "        list containing per-episode rewards and second containing per-episode lengths\n",
    "        (in number of steps).\n",
    "    \"\"\"\n",
    "\n",
    "    # Modification: Set the model to evaluation mode\n",
    "    model.evaluation_mode()\n",
    "\n",
    "    is_monitor_wrapped = False\n",
    "    # Avoid circular import\n",
    "    from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "    if not isinstance(env, VecEnv):\n",
    "        env = DummyVecEnv([lambda: env])  # type: ignore[list-item, return-value]\n",
    "\n",
    "    is_monitor_wrapped = is_vecenv_wrapped(env, VecMonitor) or env.env_is_wrapped(Monitor)[0]\n",
    "\n",
    "    if not is_monitor_wrapped and warn:\n",
    "        warnings.warn(\n",
    "            \"Evaluation environment is not wrapped with a ``Monitor`` wrapper. \"\n",
    "            \"This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. \"\n",
    "            \"Consider wrapping environment first with ``Monitor`` wrapper.\",\n",
    "            UserWarning,\n",
    "        )\n",
    "\n",
    "    n_envs = env.num_envs\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "\n",
    "    episode_counts = np.zeros(n_envs, dtype=\"int\")\n",
    "    # Divides episodes among different sub environments in the vector as evenly as possible\n",
    "    episode_count_targets = np.array([(n_eval_episodes + i) // n_envs for i in range(n_envs)], dtype=\"int\")\n",
    "\n",
    "    current_rewards = np.zeros(n_envs)\n",
    "    current_lengths = np.zeros(n_envs, dtype=\"int\")\n",
    "    observations = env.reset()\n",
    "    episode_starts = np.ones((env.num_envs,), dtype=bool)\n",
    "    while (episode_counts < episode_count_targets).any():\n",
    "\n",
    "        # Modification: Predict 1 action at a time\n",
    "        actions = np.zeros(n_envs)\n",
    "        for i, obs in enumerate(observations):\n",
    "            actions[i], _ = model.select_action(obs, greedy=True)\n",
    "        actions = convert_list_to_vectors(actions.astype(int))\n",
    "    \n",
    "        new_observations, rewards, dones, infos = env.step(actions)\n",
    "        current_rewards += rewards\n",
    "        current_lengths += 1\n",
    "        for i in range(n_envs):\n",
    "            if episode_counts[i] < episode_count_targets[i]:\n",
    "                # unpack values so that the callback can access the local variables\n",
    "                reward = rewards[i]\n",
    "                done = dones[i]\n",
    "                info = infos[i]\n",
    "                episode_starts[i] = done\n",
    "\n",
    "                if callback is not None:\n",
    "                    callback(locals(), globals())\n",
    "\n",
    "                if dones[i]:\n",
    "                    if is_monitor_wrapped:\n",
    "                        # Atari wrapper can send a \"done\" signal when\n",
    "                        # the agent loses a life, but it does not correspond\n",
    "                        # to the true end of episode\n",
    "                        if \"episode\" in info.keys():\n",
    "                            # Do not trust \"done\" with episode endings.\n",
    "                            # Monitor wrapper includes \"episode\" key in info if environment\n",
    "                            # has been wrapped with it. Use those rewards instead.\n",
    "                            episode_rewards.append(info[\"episode\"][\"r\"])\n",
    "                            episode_lengths.append(info[\"episode\"][\"l\"])\n",
    "                            # Only increment at the real end of an episode\n",
    "                            episode_counts[i] += 1\n",
    "                            # Modification: Print the progress\n",
    "                            clear_output(wait=True)\n",
    "                            print(f\"Model {combination[0]} VS Model {combination[1]} progress:({episode_counts[0]/episode_count_targets[0]*100:.2f}%)\")\n",
    "                    else:\n",
    "                        episode_rewards.append(current_rewards[i])\n",
    "                        episode_lengths.append(current_lengths[i])\n",
    "                        episode_counts[i] += 1\n",
    "                    current_rewards[i] = 0\n",
    "                    current_lengths[i] = 0\n",
    "\n",
    "        observations = new_observations\n",
    "\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "    if reward_threshold is not None:\n",
    "        assert mean_reward > reward_threshold, \"Mean reward below threshold: \" f\"{mean_reward:.2f} < {reward_threshold:.2f}\"\n",
    "    if return_episode_rewards:\n",
    "        return episode_rewards, episode_lengths\n",
    "    return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EVALUATIONS = 1000 \n",
    "LOGGING_DIR = \"Logging/EVALUATION\"\n",
    "N_CPU = 50\n",
    "SEED = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 5 VS Model 6 progress:(100.00%)\n"
     ]
    }
   ],
   "source": [
    "# Make a returns matrix\n",
    "# Last dimension is to store each training episode\n",
    "returns = np.zeros((len(models), len(models), NUM_EVALUATIONS))\n",
    "\n",
    "for i in range(len(models)):\n",
    "    for j in range(i+1, len(models)):\n",
    "\n",
    "        # Extract the models\n",
    "        agent1 = models[i][\"agent\"]\n",
    "        agent2 = models[j][\"agent\"]\n",
    "\n",
    "        # Run the evaluations\n",
    "        # Set the model in evaluation mode\n",
    "        agent1.evaluation_mode()\n",
    "        agent2.evaluation_mode()\n",
    "\n",
    "        if i == 5 or j == 5:\n",
    "            # The expert baseline is an RNN and cannot be used for multiple games in parallel without messing the hidden state propagation\n",
    "            vec_env = make_vec_env(SlimeVolleyParallelWrapper, n_envs=1, seed=SEED, env_kwargs={\"opponent_agent\": agent2})\n",
    "            \n",
    "        # Vectorize the environment\n",
    "        vec_env = make_vec_env(SlimeVolleyParallelWrapper, n_envs=N_CPU, seed=SEED, env_kwargs={\"opponent_agent\": agent2})\n",
    "\n",
    "        # Run the evaluations\n",
    "        rewards, _ = evaluate_policy(agent1, vec_env, n_eval_episodes=NUM_EVALUATIONS, return_episode_rewards=True, combination=(i,j))\n",
    "\n",
    "        # Convert the rewards to a numpy array\n",
    "        rewards = np.array(rewards)\n",
    "        \n",
    "        # Store the returns (The rewards are from the perspective of agent 1)\n",
    "        returns[i, j] = rewards\n",
    "        returns[j, i] = -rewards\n",
    "\n",
    "# Save the returns\n",
    "np.savez(f\"{LOGGING_DIR}/eval_return_results.npz\", returns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Genetic - Selfplay PPO - Expert training PPO - Selfplay  \\\n",
      "Genetic - Selfplay          0.00 +- 0.00          4.89 +- 0.35   0.08 +- 0.80   \n",
      "PPO - Expert training      -4.89 +- 0.35          0.00 +- 0.00  -3.60 +- 1.43   \n",
      "PPO - Selfplay             -0.08 +- 0.80          3.60 +- 1.43   0.00 +- 0.00   \n",
      "A2C - Expert training      -4.24 +- 1.09         -2.63 +- 2.01  -3.82 +- 1.30   \n",
      "A2C - Selfplay             -1.21 +- 1.08         -0.92 +- 0.93  -0.91 +- 0.95   \n",
      "Expert baseline            -0.38 +- 0.89         -0.44 +- 0.71  -0.39 +- 0.86   \n",
      "Random baseline            -4.93 +- 0.27         -0.47 +- 2.77  -4.74 +- 0.54   \n",
      "\n",
      "                      A2C - Expert training A2C - Selfplay Expert baseline  \\\n",
      "Genetic - Selfplay             4.24 +- 1.09   1.21 +- 1.08    0.38 +- 0.89   \n",
      "PPO - Expert training          2.63 +- 2.01   0.92 +- 0.93    0.44 +- 0.71   \n",
      "PPO - Selfplay                 3.82 +- 1.30   0.91 +- 0.95    0.39 +- 0.86   \n",
      "A2C - Expert training          0.00 +- 0.00   0.68 +- 0.95    0.10 +- 0.89   \n",
      "A2C - Selfplay                -0.68 +- 0.95   0.00 +- 0.00   -0.84 +- 1.09   \n",
      "Expert baseline               -0.10 +- 0.89   0.84 +- 1.09    0.00 +- 0.00   \n",
      "Random baseline               -0.96 +- 2.70  -4.53 +- 0.75   -4.88 +- 0.34   \n",
      "\n",
      "                      Random baseline  \n",
      "Genetic - Selfplay       4.93 +- 0.27  \n",
      "PPO - Expert training    0.47 +- 2.77  \n",
      "PPO - Selfplay           4.74 +- 0.54  \n",
      "A2C - Expert training    0.96 +- 2.70  \n",
      "A2C - Selfplay           4.53 +- 0.75  \n",
      "Expert baseline          4.88 +- 0.34  \n",
      "Random baseline          0.00 +- 0.00  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38284/4138045542.py:7: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = pd.DataFrame(mean_returns, columns=[model[\"name\"] for model in models], index=[model[\"name\"] for model in models]).applymap(lambda x: f\"{x:.2f}\")\n",
      "/tmp/ipykernel_38284/4138045542.py:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df += pd.DataFrame(std_returns, columns=[model[\"name\"] for model in models], index=[model[\"name\"] for model in models]).applymap(lambda x: f\"{x:.2f}\")\n"
     ]
    }
   ],
   "source": [
    "# Load the returns\n",
    "# returns = np.load(f\"{LOGGING_DIR}/eval_return_results.npz\")[\"arr_0\"]\n",
    "\n",
    "# Print the pairwise mean returns with standard deviation in table format with the model names as column and row headers\n",
    "mean_returns = np.mean(returns, axis=-1)\n",
    "std_returns = np.std(returns, axis=-1)\n",
    "df = pd.DataFrame(mean_returns, columns=[model[\"name\"] for model in models], index=[model[\"name\"] for model in models]).applymap(lambda x: f\"{x:.2f}\")\n",
    "df += \" +- \"\n",
    "df += pd.DataFrame(std_returns, columns=[model[\"name\"] for model in models], index=[model[\"name\"] for model in models]).applymap(lambda x: f\"{x:.2f}\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELO evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the new ELOs of both players\n",
    "def calculate_elos(elo1, elo2, s1, s2, K=32):\n",
    "\n",
    "    # Calculate the expected score\n",
    "    expected_score = 1 / (1 + 10**((elo2 - elo1) / 400))\n",
    "\n",
    "    # Calculate the new ELOs\n",
    "    new_elo1 = elo1 + K * (s1 - expected_score)\n",
    "    new_elo2 = elo2 + K * (s2 - (1 - expected_score))\n",
    "\n",
    "    # Return both ELOs\n",
    "    return new_elo1, new_elo2\n",
    "\n",
    "# Initialize the ELOs at 12000\n",
    "elos = np.zeros(len(models))\n",
    "for i in range(len(models)):\n",
    "    elos[i] = 1200\n",
    "\n",
    "# Extract the array of (agent1, agent2, s1, s2)\n",
    "# We do this to avoid replaying all the episodes\n",
    "games = [] # Divide by two cause each game is duplicated\n",
    "for i in range(len(models)):\n",
    "    for j in range(i+1, len(models)):\n",
    "        for e in range(NUM_EVALUATIONS):\n",
    "            if returns[i, j, e] > 0:\n",
    "                games.append([i, j, 1, 0])\n",
    "            elif returns[i, j, e] < 0:\n",
    "                games.append([i, j, 0, 1])\n",
    "            else:\n",
    "                games.append([i, j, 0.5, 0.5])\n",
    "\n",
    "# Shuffle the list of games and update the elos based on the results\n",
    "np.random.shuffle(games)\n",
    "for g in games:\n",
    "    i = g[0]\n",
    "    j = g[1]\n",
    "    s1 = g[2]\n",
    "    s2 = g[3]\n",
    "    elos[i], elos[j] = calculate_elos(elos[i], elos[j], s1, s2)\n",
    "\n",
    "# Save the ELOs\n",
    "np.savez(f\"{LOGGING_DIR}/eval_elos.npz\", elos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELO computation complete!\n",
      "Genetic - Selfplay ELO: 1493.3908061851723\n",
      "PPO - Expert training ELO: 1252.8278681262454\n",
      "PPO - Selfplay ELO: 1436.5233953968013\n",
      "A2C - Expert training ELO: 1054.3145207893958\n",
      "A2C - Selfplay ELO: 1092.1503951793206\n",
      "Expert baseline ELO: 1243.0168134955486\n",
      "Random baseline ELO: 827.7762008275249\n"
     ]
    }
   ],
   "source": [
    "# Load the ELOs\n",
    "# elos = np.load(f\"{LOGGING_DIR}/eval_elos.npz\")[\"arr_0\"]\n",
    "\n",
    "# Print the ELOs\n",
    "print(\"ELO computation complete!\")\n",
    "for i, model in enumerate(models):\n",
    "    print(f\"{model['name']} ELO: {elos[i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
