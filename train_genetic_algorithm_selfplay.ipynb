{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Inspired from: https://github.com/hardmaru/slimevolleygym/blob/master/training_scripts/train_ga_selfplay.py\n",
    "Trains an agent from scratch (no existing AI) using evolution\n",
    "GA with no cross-over, just mutation, and random tournament selection\n",
    "Not optimized for speed, and just uses a single CPU (mainly for simplicity)\n",
    "'''\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import gym\n",
    "import slimevolleygym\n",
    "import slimevolleygym.mlp as mlp\n",
    "from slimevolleygym.mlp import Model\n",
    "from slimevolleygym import multiagent_rollout as rollout\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from slimevolleygym import BaselinePolicy\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "random_seed = 612\n",
    "num_agents = 128\n",
    "total_num_games = 1000000\n",
    "save_freq = 1000\n",
    "logging_freq = 100 # Log to tensorboard every this many games\n",
    "num_eval_episodes = 10\n",
    "logging_dir = f\"Logging/GENETIC-SELFPLAY/{datetime.now().strftime('%Y%m%d-%H%M%S')}-numagents-{num_agents}-totalnumgames-{total_num_games}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If random=True, we will use a random agent (Otherwise we use the baseline)\n",
    "def evaluate(env, model_params, num_eval_episodes, random=False):\n",
    "\n",
    "    # Load the model with the params\n",
    "    policy = Model(mlp.games['slimevolleylite'])\n",
    "    policy.set_model_params(model_params)\n",
    "\n",
    "    if not random:\n",
    "        opponent = BaselinePolicy()\n",
    "    \n",
    "    # Run num_eval_episodes episodes and calculate the total return\n",
    "    total_return = 0\n",
    "    for _ in range(num_eval_episodes):\n",
    "\n",
    "        state1 = env.reset()\n",
    "        state2 = state1\n",
    "        done = False\n",
    "        while not done:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Select the actions for each agent\n",
    "                # Setting mean_mode=True to avoid any randomness\n",
    "                action1 = policy.predict(state1, mean_mode=True)\n",
    "\n",
    "                if not random:\n",
    "                    action2 = opponent.predict(state2)\n",
    "                else:\n",
    "                    action2 = env.action_space.sample()\n",
    "            \n",
    "            # Step the environment forward\n",
    "            next_state1, reward, done, info = env.step(action1, otherAction=action2)\n",
    "            next_state2 = info['otherObs']\n",
    "            \n",
    "            # Add the individual agents' rewards to the total returns (Since they're the same for both agents)\n",
    "            total_return += reward\n",
    "\n",
    "            # Update the states\n",
    "            state1 = next_state1\n",
    "            state2 = next_state2\n",
    "\n",
    "    # Return the average return\n",
    "    return total_return / num_eval_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_880/4001767669.py:5: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  policy_left = Model(mlp.games['slimevolleylite'])\n",
      "/tmp/ipykernel_880/4001767669.py:6: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  policy_right = Model(mlp.games['slimevolleylite'])\n",
      "  0%|                                                                                                                                 | 0/1000000 [00:00<?, ?it/s]/tmp/ipykernel_880/4001767669.py:26: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  policy_left.set_model_params(population[m])\n",
      "/tmp/ipykernel_880/4001767669.py:27: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  policy_right.set_model_params(population[n])\n",
      " 49%|█████████████████████████████████████████████████████▏                                                       | 487402/1000000 [30:05:04<112:40:30,  1.26it/s]"
     ]
    }
   ],
   "source": [
    "# Create a writer\n",
    "writer = SummaryWriter(logging_dir)\n",
    "\n",
    "# Create two instances of a feed forward policy we may need later.\n",
    "policy_left = Model(mlp.games['slimevolleylite'])\n",
    "policy_right = Model(mlp.games['slimevolleylite'])\n",
    "param_count = policy_left.param_count\n",
    "\n",
    "# Store our population here\n",
    "population = np.random.normal(size=(num_agents, param_count)) * 0.5 # each row is an agent.\n",
    "winning_streak = [0] * num_agents # store the number of wins for this agent (including mutated ones)\n",
    "\n",
    "# Create the gym environment, and seed it\n",
    "env = slimevolleygym.SlimeVolleyEnv()\n",
    "env.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Store the history of the length of the games\n",
    "history = []\n",
    "\n",
    "# Run total_num_games games\n",
    "for game in tqdm(range(1, total_num_games+1)):\n",
    "\n",
    "  # Randomly extract 2 agents from the population\n",
    "  m, n = np.random.choice(num_agents, 2, replace=False)\n",
    "  policy_left.set_model_params(population[m])\n",
    "  policy_right.set_model_params(population[n])\n",
    "\n",
    "  # Run a game between them\n",
    "  score, length = rollout(env, policy_right, policy_left)\n",
    "  \n",
    "  # Append the length of the game to the history\n",
    "  history.append(length)\n",
    "\n",
    "  # If score is positive, it means policy_right won.\n",
    "  if score == 0: # If the game is tied, add noise to the left agent.\n",
    "    population[m] += np.random.normal(size=param_count) * 0.1\n",
    "  if score > 0:\n",
    "    population[m] = population[n] + np.random.normal(size=param_count) * 0.1\n",
    "    winning_streak[m] = winning_streak[n]\n",
    "    winning_streak[n] += 1\n",
    "  if score < 0:\n",
    "    population[n] = population[m] + np.random.normal(size=param_count) * 0.1\n",
    "    winning_streak[n] = winning_streak[m]\n",
    "    winning_streak[m] += 1\n",
    "  \n",
    "  # Save the agent with the longest winning streak\n",
    "  if game % save_freq == 0:\n",
    "    model_filename = f\"{logging_dir}/game_{game}\"\n",
    "    file = open(model_filename, \"x\")\n",
    "    f = open(model_filename, \"w\")\n",
    "    record_holder = np.argmax(winning_streak)\n",
    "    record = winning_streak[record_holder]\n",
    "    json.dump([population[record_holder].tolist(), record], f, sort_keys=True, indent=0, separators=(',', ': '))\n",
    "    f.close()\n",
    "  \n",
    "  # Log the winning streak of the best agent\n",
    "  if game % logging_freq == 0:\n",
    "    \n",
    "    # Extract the best agent as well as their winning streak\n",
    "    record_holder = np.argmax(winning_streak)\n",
    "    record = winning_streak[record_holder]\n",
    "\n",
    "    # Log the winning streak of the best agent as a function of the game number\n",
    "    writer.add_scalar('Best winning streak - Game', record, game)\n",
    "\n",
    "    # Log the mean duration of the games as a function of the game number\n",
    "    writer.add_scalar('Average game duration - Game', np.mean(history), game)\n",
    "    writer.add_scalar('Game duration standard deviation - Game', np.std(history), game)\n",
    "\n",
    "    # Run a few games between the best agent and a random agent\n",
    "    random_score = evaluate(env, population[record_holder], num_eval_episodes, random=True)\n",
    "\n",
    "    # Run a few games between the best agent and the baseline agent\n",
    "    baseline_score = evaluate(env, population[record_holder], num_eval_episodes, random=False)\n",
    "\n",
    "    # Log both scores\n",
    "    writer.add_scalar('Best agent vs Random agent returns - Game', random_score, game)\n",
    "    writer.add_scalar('Best agent vs Baseline agent returns - Game', baseline_score, game)\n",
    "\n",
    "    # Reset the history\n",
    "    history = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
