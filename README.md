# SlimeVolleyball

  This study consisted of exploring the two main methods for training 1v1 agents, expert training and selfplay, on the SlimeVolleyball gym environment. The algorithms implemented are PPO, DDQN, A2C, and a genetic approach. By ranking the agents using their ELO scores, we are able to conclude that selfplay leads to more flexible agents, and that PPO is the easiest to train out of the RL agents. We also observed the evolution of the selfplay strategies across generations to better understand the way agents learn an optimal strategy.